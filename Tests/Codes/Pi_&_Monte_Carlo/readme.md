# Pi Calculation with MPI, Slurm and the Monte Carlo Method

![Language](https://img.shields.io/badge/Linguagem-C-blue.svg)
![Parallelis](https://img.shields.io/badge/Framework-MPI-orange.svg)
![Cluster](https://img.shields.io/badge/Gerenciador-Slurm-red.svg)

This project demonstrates how to calculate the value of Pi (Ï€) using the Monte Carlo statistical method. The implementation is done in C and uses the MPI (Message Passing Interface) standard to parallelize the calculation and execute it efficiently on a computer cluster.

## The Monte Carlo Method for Pi

The idea is to simulate randomly throwing "darts" at a 2x2 square, which has a circle of radius 1 inscribed within it. The area of the square is $2^2 = 4$, and the area of the circle is $\pi \cdot 1^2 = \pi$.

The ratio of the circle's area to the square's area is $\frac{\pi}{4}$. Therefore, if we throw a very large number of darts, the proportion of darts that land inside the circle relative to the total number of darts thrown will approximate $\frac{\pi}{4}$.

From there, we can estimate the value of Pi:

$$\pi \approx 4 \cdot \frac{\text{hits inside the circle}}{\text{total darts thrown}}$$

## The Challenge and the Solution with Parallelism

For the approximation to be accurate, we need a gigantic number of "darts"â€”on the order of billions. Performing this task on a single computer would be extremely slow. The solution is to use parallelism. To make the calculation feasible, the work is divided among multiple computer processes. MPI is a communication standard that allows these processes, running on different machines (nodes of a cluster), to exchange data and cooperate.

### Parallelization Strategy: An Analogy

To understand how this division works in practice, we can imagine a team of helpers hired to perform the dart game:

* **Task Division:** The total work (e.g., 1 billion darts) is divided equally among the number of available helpers. In the code, each process simply calculates its quota with the line `shots_per_process = total_shots / num_procs;`.
* **Unique and Parallel Execution:** Each helper starts throwing their quota of darts at the same time as the others. The crucial point is that each one has a unique "throwing technique," ensuring they do not duplicate work. In the code, this is guaranteed because each process initializes its random number generator with a unique "seed" (`srand(time(NULL) + rank);`), causing the `(x, y)` points generated by `rand()` to be different for each one.
* **Result Aggregation:** In the end, a team leader asks each helper how many hits they got and sums them all to get the grand total. This step of "summing everything" is performed efficiently by the `MPI_Reduce(&local_hits, &global_hits, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);` function from MPI, which collects the `local_hits` from everyone and sums them in the master process (rank 0).

### File Descriptions

ðŸ“„ **`monte_carlo_mpi.c`**

* The core of the calculation logic, written in C.
* **MPI Initialization:** Sets up the environment for communication between processes with `MPI_Init`.
* **Load Balancing:** The total number of "shots" is divided equally among all allocated processes (`total_shots / num_procs`).
* **Independent Calculation:** Each process runs its own Monte Carlo simulation in a `for` loop.
* **Distinct Random Seeds:** Ensures statistical independence of the results by using the process `rank` to generate a unique seed for the `srand()` function.
* **Result Aggregation:** Uses `MPI_Reduce` to sum the hits (`local_hits`) from all processes and consolidate the result (`global_hits`) in the master process (rank 0).
* **Formatted Output:** Only the master process (`rank == 0`) prints the final result, avoiding duplicate outputs.

ðŸ“œ **`pi_mpi_flexivel.sh`**

* A submission script for Slurm that automates execution on the cluster.
* **Slurm Directives:** The `#SBATCH` lines configure the job, defining its name, log files, execution time, and, most importantly, the number of tasks per node (`--ntasks-per-node=1`).
* **Flexibility:** Allows the user to define the number of nodes on the command line (`sbatch --nodes=...`).
* **Workflow Automation:**
    * Creates a secure and isolated working directory for each job.
    * Copies the source code to the working directory.
    * Compiles the C code using `mpicc` with the `-O3` optimization flag.
    * Executes the compiled program with `mpirun`, which seamlessly integrates with Slurm to distribute the processes across the allocated nodes.
    * Measures and reports the total script execution time.

### ðŸš€ How to Use (on a Slurm Cluster)

**1. Prerequisites**

The `pi_mpi_flexivel.sh` script expects to find the C source code at a specific location. Ensure the file is in the correct path or adjust the `SOURCE_FILE` variable in the script.

**2. Job Submission**

Use the `sbatch` command to submit the script to the Slurm queue. You must specify:
* `--nodes`: The number of compute nodes you wish to use.
* `<number_of_shots>`: The argument for the script, which represents the total workload.

**Syntax:**
```bash
sbatch --nodes=<number_of_nodes> pi_mpi_flexivel.sh <total_number_of_shots>
```

**Practical Example:** To run the calculation on 16 nodes with 100 billion shots:
```bash
sbatch --nodes=16 pi_mpi_flexivel.sh 100000000000
```
**Attention:** The total number of shots must be divisible by the number of processes (nodes, in this case).

**3. Analyzing Results**

The output and error logs will be generated in the `/nfs/return/out/` and `/nfs/return/err/` directories, respectively, with the Job ID (`$SLURM_JOB_ID`) in the filename.

### ðŸ’» How to Run Locally (Without Slurm)

If you have an MPI implementation (like Open MPI or MPICH) installed on your local machine, you can compile and run the program directly.

**1. Compilation**

Use `mpicc` to compile the program. The command below uses `-O3` optimization.
```bash
mpicc monte_carlo_mpi.c -o pi_exec_monte_carlo -O3 -lm
```

**2. Execution**

Use `mpirun` with the `-np` flag to specify the number of processes.
**Syntax:**
```bash
mpirun -np <number_of_processes> ./pi_exec_monte_carlo <total_number_of_shots>
```
**Practical Example:** To run with 4 processes and 1 billion shots:
```bash
mpirun -np 4 ./pi_exec_monte_carlo 1000000000
```

### ðŸ“Š Output Example

The program's output, which will be printed by the rank 0 process, will have the following format:

```
====================================================
Calculation of Pi with MPI and Monte Carlo Method
----------------------------------------------------
Number of MPI processes...: 16
Total shots...............: 100000000000
Total hits................: 78539815000
Calculation execution time: 152.731982 seconds
Estimated value of Pi.....: 3.141592600000000
====================================================
```