# C√°lculo de Pi com MPI e Slurm

![Linguagem](https://img.shields.io/badge/Linguagem-C-blue.svg)
![Paralelismo](https://img.shields.io/badge/Framework-MPI-orange.svg)
![Cluster](https://img.shields.io/badge/Gerenciador-Slurm-red.svg)

Este projeto oferece uma implementa√ß√£o em C e MPI para calcular o valor de Pi (œÄ) atrav√©s do m√©todo estat√≠stico de Monte Carlo. O sistema √© projetado para ser executado de forma eficiente em clusters de computa√ß√£o de alto desempenho (HPC) gerenciados pelo Slurm Workload Manager.

## üéØ Conceitos Chave

### M√©todo de Monte Carlo para Pi
A ideia √© simular o lan√ßamento aleat√≥rio de "dardos" em um quadrado de lado 2, que cont√©m um c√≠rculo de raio 1 inscrito nele. A √°rea do quadrado √© $2^2 = 4$, e a √°rea do c√≠rculo √© $\pi \cdot 1^2 = \pi$.

A raz√£o entre a √°rea do c√≠rculo e a √°rea do quadrado √© $\frac{\pi}{4}$. Portanto, se lan√ßarmos um n√∫mero muito grande de dardos, a propor√ß√£o de dardos que caem dentro do c√≠rculo em rela√ß√£o ao total de dardos lan√ßados ser√° uma aproxima√ß√£o de $\frac{\pi}{4}$.

$$\pi \approx 4 \cdot \frac{\text{acertos dentro do c√≠rculo}}{\text{total de dardos lan√ßados}}$$

### Paralelismo com MPI (Message Passing Interface)
Para tornar o c√°lculo vi√°vel com bilh√µes de "dardos", o trabalho √© dividido entre m√∫ltiplos processos. O MPI √© um padr√£o de comunica√ß√£o que permite que esses processos, executando em diferentes n√≥s de um cluster, troquem dados. Neste projeto, o trabalho √© dividido, cada processo calcula uma parte, e os resultados s√£o somados no final (`MPI_Reduce`).

## üìÇ Descri√ß√£o dos Arquivos

### üìÑ `monte_carlo_mpi.c`
O n√∫cleo da l√≥gica de c√°lculo, escrito em C.

* **Inicializa√ß√£o MPI**: Configura o ambiente para comunica√ß√£o entre processos.
* **Divis√£o de Carga**: O n√∫mero total de "tiros" √© dividido igualmente entre todos os processos alocados (`total_tiros / num_procs`).
* **C√°lculo Independente**: Cada processo executa sua pr√≥pria simula√ß√£o de Monte Carlo.
* **Sementes Aleat√≥rias Distintas**: Garante a independ√™ncia estat√≠stica dos resultados usando o `rank` do processo para gerar uma semente √∫nica para a fun√ß√£o `srand()`.
* **Agrega√ß√£o de Resultados**: Utiliza `MPI_Reduce` para somar os acertos de todos os processos e consolidar o resultado no processo mestre (rank 0).
* **Sa√≠da Formatada**: Apenas o processo mestre imprime o resultado final, evitando sa√≠das duplicadas.

### üìú `pi_mpi_flexivel.sh`
Um script de submiss√£o para o Slurm que automatiza a execu√ß√£o no cluster.

* **Diretivas Slurm**: As linhas `#SBATCH` configuram o job, definindo nome, arquivos de log, tempo de execu√ß√£o e, mais importante, o n√∫mero de tarefas por n√≥ (`--ntasks-per-node=1`).
* **Flexibilidade**: Permite que o usu√°rio defina o n√∫mero de n√≥s na linha de comando (`sbatch --nodes=...`).
* **Automa√ß√£o do Fluxo de Trabalho**:
    1.  Cria um diret√≥rio de trabalho seguro e isolado para cada job.
    2.  Copia o c√≥digo-fonte para o diret√≥rio de trabalho.
    3.  Compila o c√≥digo C usando `mpicc` com o flag de otimiza√ß√£o `-O3`.
    4.  Executa o programa compilado com `mpirun`, que se integra perfeitamente ao Slurm para distribuir os processos pelos n√≥s alocados.
    5.  Mede e reporta o tempo total de execu√ß√£o.

## üöÄ Como Usar (em um Cluster com Slurm)

### 1. Pr√©-requisitos
O script `pi_mpi_flexivel.sh` espera encontrar o c√≥digo-fonte C em um local espec√≠fico. Garanta que o arquivo esteja no caminho correto ou ajuste o script:
```bash
# Caminho definido no script pi_mpi_flexivel.sh
SOURCE_FILE="/nfs/execution/monte_carlo_mpi.c"
```

### 2. Submiss√£o do Job
Use o comando `sbatch` para enviar o script para a fila do Slurm. Voc√™ deve especificar:
-   `--nodes`: O n√∫mero de n√≥s de computa√ß√£o que deseja usar.
-   `<numero_total_de_tiros>`: O argumento para o script, que representa a carga de trabalho total.

**Sintaxe:**
```bash
sbatch --nodes=<numero_de_nos> pi_mpi_flexivel.sh <numero_total_de_tiros>
```

**Exemplo Pr√°tico:**
Para executar o c√°lculo em **16 n√≥s** com **100 bilh√µes de tiros**:
```bash
sbatch --nodes=16 pi_mpi_flexivel.sh 100000000000
```
**Aten√ß√£o:** O n√∫mero total de tiros deve ser divis√≠vel pelo n√∫mero de n√≥s.

### 3. An√°lise dos Resultados
Os logs de sa√≠da e erro ser√£o gerados nos diret√≥rios `/nfs/return/out/` e `/nfs/return/err/`, respectivamente, com o ID do Job no nome do arquivo.

## üí° Recomenda√ß√£o de Carga de Trabalho

Para resultados com maior precis√£o e para realmente testar a capacidade do cluster, √© recomendado usar um n√∫mero de "tiros" (tentativas) muito alto.

> **Recomenda-se iniciar os testes com valores acima de 9 bilh√µes de tiros.**

Quanto maior o n√∫mero de tiros, mais o resultado se aproximar√° do valor real de œÄ, e mais evidente ser√° o benef√≠cio da computa√ß√£o paralela na redu√ß√£o do tempo de execu√ß√£o.

## üíª Como Executar Localmente (Sem Slurm)

Se voc√™ tiver uma implementa√ß√£o do MPI (como Open MPI ou MPICH) instalada em sua m√°quina local, pode compilar e executar o programa diretamente.

### 1. Compila√ß√£o
Use o `mpicc`, que √© um wrapper para o seu compilador C (como o GCC), para compilar o programa.
```bash
mpicc monte_carlo_mpi.c -o pi_exec_monte_carlo -O3 -lm
```

### 2. Execu√ß√£o
Use `mpirun` ou `mpiexec` para executar o programa compilado.
-   Use o flag `-np` para especificar o n√∫mero de processos que deseja simular.

**Sintaxe:**
```bash
mpirun -np <numero_de_processos> ./pi_exec_monte_carlo <numero_total_de_tiros>
```

**Exemplo Pr√°tico:**
Para executar com **4 processos** e **1 bilh√£o de tiros**:
```bash
mpirun -np 4 ./pi_exec_monte_carlo 1000000000
```

## üìä Exemplo de Sa√≠da
A sa√≠da do programa, que ser√° encontrada no arquivo de log do Slurm, ter√° o seguinte formato:
```
====================================================
Calculation of Pi with MPI and Monte Carlo Method
----------------------------------------------------
Number of MPI processes...: 16
Total shots............: 100000000000
Total hits..........: 78539815000
Calculation execution time: 152.731982 segundos
Estimated value of Pi......: 3.141592600000000
====================================================
```
