# C√°lculo de Pi com MPI e o M√©todo de Monte Carlo

Este projeto demonstra como calcular o valor de Pi (œÄ) utilizando o m√©todo estat√≠stico de Monte Carlo. A implementa√ß√£o √© feita em C e utiliza o padr√£o MPI (Message Passing Interface) para paralelizar o c√°lculo e execut√°-lo de forma eficiente em um cluster de computadores.

## O M√©todo de Monte Carlo para Pi

A ideia √© simular o lan√ßamento aleat√≥rio de "dardos" em um quadrado de lado 2, que cont√©m um c√≠rculo de raio 1 inscrito nele. A √°rea do quadrado √© $2^2 = 4$, e a √°rea do c√≠rculo √© $\pi \cdot 1^2 = \pi$.

A raz√£o entre a √°rea do c√≠rculo e a √°rea do quadrado √© $\frac{\pi}{4}$. Portanto, se lan√ßarmos um n√∫mero muito grande de dardos, a propor√ß√£o de dardos que caem dentro do c√≠rculo em rela√ß√£o ao total de dardos lan√ßados ser√° uma aproxima√ß√£o de $\frac{\pi}{4}$.

A partir da√≠, podemos estimar o valor de Pi:

$$\pi \approx 4 \cdot \frac{\text{acertos dentro do c√≠rculo}}{\text{total de dardos lan√ßados}}$$

## O Desafio e a Solu√ß√£o com Paralelismo

Para que a aproxima√ß√£o seja precisa, precisamos de um n√∫mero gigantesco de "dardos", na casa dos bilh√µes. Realizar essa tarefa em um √∫nico computador seria extremamente lento. A solu√ß√£o √© usar paralelismo. Para tornar o c√°lculo vi√°vel, o trabalho √© dividido entre m√∫ltiplos processos de computador. O MPI √© um padr√£o de comunica√ß√£o que permite que esses processos, executando em diferentes m√°quinas (n√≥s de um cluster), troquem dados e cooperem.

### Estrat√©gia de Paraleliza√ß√£o: Uma Analogia

Para entender como essa divis√£o funciona na pr√°tica, podemos imaginar uma equipe de ajudantes contratada para realizar o jogo de dardos:

* **Divis√£o da Tarefa:** O trabalho total (ex: 1 bilh√£o de dardos) √© dividido igualmente entre o n√∫mero de ajudantes dispon√≠veis. No c√≥digo, cada processo simplesmente calcula sua cota com a linha `shots_per_process = total_shots / num_procs;`.
* **Execu√ß√£o √önica e Paralela:** Cada ajudante come√ßa a jogar sua cota de dardos ao mesmo tempo que os outros. O ponto crucial √© que cada um tem uma "t√©cnica de arremesso" √∫nica, garantindo que n√£o dupliquem o trabalho. No c√≥digo, isso √© assegurado porque cada processo inicializa seu gerador de n√∫meros aleat√≥rios com uma "semente" √∫nica (`srand(time(NULL) + rank);`), fazendo com que os pontos `(x, y)` gerados por `rand()` sejam diferentes para cada um.
* **Agrega√ß√£o dos Resultados:** Ao final, um l√≠der de equipe pergunta a cada ajudante quantos acertos ele teve e soma tudo para obter o total geral. Essa etapa de "somar tudo" √© realizada eficientemente pela fun√ß√£o `MPI_Reduce(&local_hits, &global_hits, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);` do MPI, que coleta os `local_hits` de todos e os soma no processo mestre (rank 0).

### Descri√ß√£o dos Arquivos

üìÑ **`monte_carlo_mpi.c`**

* O n√∫cleo da l√≥gica de c√°lculo, escrito em C.
* **Inicializa√ß√£o MPI:** Configura o ambiente para comunica√ß√£o entre processos com `MPI_Init`.
* **Divis√£o de Carga:** O n√∫mero total de "tiros" √© dividido igualmente entre todos os processos alocados (`total_shots / num_procs`).
* **C√°lculo Independente:** Cada processo executa sua pr√≥pria simula√ß√£o de Monte Carlo em um la√ßo `for`.
* **Sementes Aleat√≥rias Distintas:** Garante a independ√™ncia estat√≠stica dos resultados usando o `rank` do processo para gerar uma semente √∫nica para a fun√ß√£o `srand()`.
* **Agrega√ß√£o de Resultados:** Utiliza `MPI_Reduce` para somar os acertos (`local_hits`) de todos os processos e consolidar o resultado (`global_hits`) no processo mestre (rank 0).
* **Sa√≠da Formatada:** Apenas o processo mestre (`rank == 0`) imprime o resultado final, evitando sa√≠das duplicadas.

üìú **`pi_mpi_flexivel.sh`**

* Um script de submiss√£o para o Slurm que automatiza a execu√ß√£o no cluster.
* **Diretivas Slurm:** As linhas `#SBATCH` configuram o job, definindo nome, arquivos de log, tempo de execu√ß√£o e o n√∫mero de tarefas por n√≥ (`--ntasks-per-node=1`).
* **Flexibilidade:** Permite que o usu√°rio defina o n√∫mero de n√≥s na linha de comando (`sbatch --nodes=...`).
* **Automa√ß√£o do Fluxo de Trabalho:**
    * Cria um diret√≥rio de trabalho seguro e isolado para cada job.
    * Copia o c√≥digo-fonte para o diret√≥rio de trabalho.
    * Compila o c√≥digo C usando `mpicc` com o flag de otimiza√ß√£o `-O3`.
    * Executa o programa compilado com `mpirun`, que se integra ao Slurm para distribuir os processos pelos n√≥s alocados.
    * Mede e reporta o tempo total de execu√ß√£o do script.

### üöÄ Como Usar (em um Cluster com Slurm)

**1. Pr√©-requisitos**

O script `pi_mpi_flexivel.sh` espera encontrar o c√≥digo-fonte C em um local espec√≠fico. Garanta que o arquivo esteja no caminho correto ou ajuste a vari√°vel `SOURCE_FILE` no script.

**2. Submiss√£o do Job**

Use o comando `sbatch` para enviar o script para a fila do Slurm. Voc√™ deve especificar:
* `--nodes`: O n√∫mero de n√≥s de computa√ß√£o que deseja usar.
* `<number_of_shots>`: O argumento para o script, que representa a carga de trabalho total.

**Sintaxe:**
```bash
sbatch --nodes=<numero_de_nos> pi_mpi_flexivel.sh <number_of_shots>
```

**Exemplo Pr√°tico:** Para executar o c√°lculo em 16 n√≥s com 100 bilh√µes de tiros:
```bash
sbatch --nodes=16 pi_mpi_flexivel.sh 100000000000
```
**Aten√ß√£o:** O n√∫mero total de tiros deve ser divis√≠vel pelo n√∫mero de processos (n√≥s, neste caso).

**3. An√°lise dos Resultados**

Os logs de sa√≠da e erro ser√£o gerados nos diret√≥rios `/nfs/return/out/` e `/nfs/return/err/`, respectivamente, com o ID do Job (`$SLURM_JOB_ID`) no nome do arquivo.

### üíª Como Executar Localmente (Sem Slurm)

Se voc√™ tiver uma implementa√ß√£o do MPI (como Open MPI ou MPICH) instalada, pode compilar e executar o programa.

**1. Compila√ß√£o**

Use o `mpicc` para compilar o programa. O comando abaixo usa otimiza√ß√£o `-O3`.
```bash
mpicc monte_carlo_mpi.c -o pi_exec_monte_carlo -O3 -lm
```

**2. Execu√ß√£o**

Use `mpirun` com o flag `-np` para especificar o n√∫mero de processos.
**Sintaxe:**
```bash
mpirun -np <numero_de_processos> ./pi_exec_monte_carlo <numero_total_de_tiros>
```
**Exemplo Pr√°tico:** Para executar com 4 processos e 1 bilh√£o de tiros:
```bash
mpirun -np 4 ./pi_exec_monte_carlo 1000000000
```

### üìä Exemplo de Sa√≠da

A sa√≠da do programa, que ser√° impressa pelo processo de rank 0, ter√° o seguinte formato:

```
====================================================
Calculation of Pi with MPI and Monte Carlo Method
----------------------------------------------------
Number of MPI processes...: 16
Total shots...............: 100000000000
Total hits................: 78539815000
Calculation execution time: 152.731982 seconds
Estimated value of Pi.....: 3.141592600000000
====================================================
```